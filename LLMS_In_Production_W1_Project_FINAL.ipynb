{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foopoiuyt/uplimit/blob/main/LLMS_In_Production_W1_Project_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataAlchemy Labs\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1jLQB9IRtp4cHnIxRSebSVcclKxNfdHCf\" />\n",
        "\n",
        "Welcome to the week 1 project for LLMs in Production. In this weeks project you are a founding Machine Learning Engineer on the team you and your team have narrowed down a life-changing product. You will be building and launching your groundbreaking SaaS product which is quite similar to a product already out in the market, [AI2sql](https://www.ai2sql.io/)."
      ],
      "metadata": {
        "id": "j8srXs0p12CE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![AI2sql Example](https://drive.google.com/uc?id=1mWOQs6OZmuCLui-auLosXsXGhwl2mbVg)\n",
        "\n",
        "You have done a significant amount of market research and are confident that this product will gather a cult-like following with a killer feature of being able to take any natural language query and output SQL code based on the query.\n",
        "\n",
        "Potential users of your application are:\n",
        "- **Project Managers**: They can use our tool to quickly derive important metrics for presentations or reports. Instead of relying on technical teams, they can directly query the database using natural language.\n",
        "- **Data Scientists**: Often dealing with complex data queries, data scientists can use our tool to streamline and debug intricate SQL statements, saving time and reducing the potential for error.\n",
        "- **Business Analysts and Non-Tech Professionals**: Anyone without deep SQL knowledge but needing to interact with the database can benefit immensely. They can easily convert their data needs into SQL queries without delving into the complexities of SQL syntax.\n",
        "\n",
        "There are many more features that you can work on in the future such as:\n",
        "1. NoSQL Code Generation\n",
        "2. SQL Syntax Checking\n",
        "3. Explaining SQL Code\n",
        "4. Optimizing SQL Code\n",
        "5. Formatting SQL Code\n",
        "\n",
        "But since this is an MVP and you want to laucnh as quickly as possible we will be just focussing on the use case of SQL Code Generation. In particular we will be supporting the use of **SQL** initially as its the most popular database out there in use currently based on the recent [StackOverflow Developer Survey 2023](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-databases).\n",
        "\n",
        "\n",
        "\n",
        "Our final product will be a simple web application with a basic Q/A system that allows users to ask query about SQL and then get back SQL code that they can use for their application, think of it as a super basic version of the ChatGPT from OpenAI that I am sure everyone is quite familiar with.\n",
        "\n",
        "![ChatGPT OpenAI Example](https://drive.google.com/uc?id=1AGVZoxtWvBF6KLX0mkvpnU7mMEgaSraa)\n",
        "\n",
        "In the Week 1 Project we will be solidfying what we learnt throughout the week by building out this core functionality for the SQL Code Generation part of our application. Just like all Data Science projects we start off in a notebook environment to prototype things before having to bring them to production.\n",
        "\n",
        "Just like most LLM startups we will be leveraging OpenAI and their ChatGPT API initially for our product, in particular we will be using **gpt-4o-mini** which offers a good enough LLM that is quite fast!\n",
        "\n",
        "The main parts that we will be covering in the project are:\n",
        "1. [Using LLMs to Generate SQL Code](#scrollTo=HcGIbmPiM4o3)\n",
        "2. [Evaluating LLMs on SQL Code Generation](#scrollTo=pWz1btwOM7zr)\n",
        "3. [Validating LLM Outputs using Guardrails](#scrollTo=rPmPTQssNDUn)\n",
        "4. [Extra Credit: Optional Tasks](#scrollTo=UL-XI8PiNU1z&line=1&uniqifier=1)\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/10y3EFKr4S8TsiSp00y8_LTD-2UU8nCGS?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6lSc2PMKFPyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up OpenAI API Key"
      ],
      "metadata": {
        "id": "9MduGWtPIPTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  padding: 10px;\n",
        "  border-radius: 5px;\n",
        "  background-color: #ffcccc;\n",
        "  border-left: 6px solid #ff0000;\n",
        "  margin-bottom: 20px;\">\n",
        "  \n",
        "  <strong>⚠️ Important Notice:</strong>\n",
        "  <p>Do not share or use this API Key outside of the context of the notebook exercises.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "q1v3j1VcIWFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uplimit has provisioned an OpenAI API Key for your projects. Please add this API Key to this assignment by clicking on the Security Key icon on the left hand tab of the Google Colab notebook and then add a new parameter value called `OPENAI_API_KEY`.\n",
        "\n",
        "\n",
        " Here you can provide the API key that you copied and this will not be part of your Google Colab account. You can also enable the toggle Notebook access - this will allow your notebook to have access to this API key.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1SfE-nNOQ3DfZpJN6mAnxuqVsMv-F1_Yp\" />\n",
        "\n",
        "**NOTE:** We are hardcoding an API key for [Guardrails.ai](https://www.guardrailsai.com/) which is used for downloading different types of LLM Guardrails, access is free regardless and we hardcode it as a convenience for all the students."
      ],
      "metadata": {
        "id": "RzMCOyXfIZCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the API Key has been setup, run the following code:"
      ],
      "metadata": {
        "id": "nMMB_lzHIdpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai guardrails-ai==0.5.0 deepeval langchain langchain-openai"
      ],
      "metadata": {
        "id": "vcOQFQC7jUU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GUARDRAILS_TOKEN\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwNzI3MDE2NDYwNDM4MTg2OTU4OSIsImFwaUtleUlkIjoiYjAzNDVkNGEtZDhjNy00OThmLWIwZGYtZWI3ZTY0MzMwNTJkIiwiaWF0IjoxNzMwMTQ2ODc0LCJleHAiOjE3Mzc5MjI4NzR9.1J84O3pT_KlWTkGjV4zfBMChqYow868A-XTsePwiZ_Q\""
      ],
      "metadata": {
        "id": "OFBxDLxcIViM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Using LLMs to Generate SQL Code\n",
        "\n",
        "## SQL Code Generation\n",
        "\n",
        "SQL is one of the most powerful programming languages out there and is the main way in which we communicate with databases. Without it backend engineers would be unable to store and structure data to be easily queried to return to a frontend application and data scientists would be unable to track key metrics about their experiments over time.\n",
        "\n",
        "SQL is the language used for interacting with databases, while SQL is a specific database system that understands and uses SQL for database operations. Each database system, including SQL, implements SQL with some variations and adds its own proprietary extensions to the standard SQL language.\n",
        "\n",
        "Here is an example SQL statement:\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "    u.name,\n",
        "    u.email,\n",
        "    COUNT(o.order_id) AS total_orders,\n",
        "    SUM(o.amount) AS total_amount_spent\n",
        "FROM\n",
        "    users u\n",
        "JOIN\n",
        "    orders o ON u.user_id = o.user_id\n",
        "WHERE\n",
        "    o.order_date >= (NOW() - INTERVAL '1 year')\n",
        "GROUP BY\n",
        "    u.user_id\n",
        "HAVING\n",
        "    COUNT(o.order_id) > 5\n",
        "ORDER BY\n",
        "    total_amount_spent DESC;\n",
        "```\n",
        "\n",
        "LLM's such as ChatGPT from OpenAI are trained on all kinds of data across the internet and in particular they are trained on code across many different programming languages.\n",
        "\n",
        "\n",
        "![Text2SQL](https://drive.google.com/uc?id=17IDU11L_JdPIfMWvDa6li_5oOyGgWriX)\n",
        "\n",
        "What we will be performing is asking the LLM a Question related to a SQL query in order to create SQL code that we can then run.\n",
        "\n",
        "\n",
        "During OpenAI DevDay, there was a fantastic breakout section about **\"[A Survey of Techniques for Maximizing LLM Performance](https://youtu.be/ahnGLM-RC1Y?si=rifoSUxFgvgliFtT)\"** which we recommend all of you go through. In that talk, the speakers highlighted how we should always be starting off with Prompt Engineering when building LLM applications. Once we understand what our LLM application lacks can we only try more complex techniques such as RAG or Fine-tuning or both. With that spirit in mind let's try out some basic Prompt Engineering using ChatGPT.\n",
        "\n",
        "![Optimization Flow from OpenAI](https://drive.google.com/uc?id=1kcguLT8KYBmDypGB0fVyeT8YfrCiiMx_)"
      ],
      "metadata": {
        "id": "HcGIbmPiM4o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import LLMResult, HumanMessage, Generation\n",
        "\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=MODEL_NAME,\n",
        "    temperature=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "82LGwE57Zc4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Create a Baseline Zero-Shot Prompt\n",
        "\n",
        "Create a Zero-Shot Prompt that we can use in this situation, we have provided you with a simple prompt to try out. A **Zero-Shot prompt** is where we are asking the model to perform a task it has never seen explicty during training without any additional examples.\n",
        "\n",
        "When working on the prompt or improving the prompt remember a couple of things:\n",
        "\n",
        "1. Make sure to use clear instructions\n",
        "2. Try splitting tasks into simpler subtasks\n",
        "3. Give the model time to “think”\n",
        "\n",
        "![How to Prompt Engineer](https://drive.google.com/uc?id=1iSZ6XvD5eMiRSQBUT0DFEDA-1qc0qibv)\n"
      ],
      "metadata": {
        "id": "RE5HXufjHF8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Add your prompt here, make sure to use\n",
        "# {query} to inject a query into the prompt.\n",
        "\n",
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "You will be given a prompt which you will have to translate to SQL.\n",
        "Here is the query: {query}\n",
        "\"\"\"\n",
        "\n",
        "# Try your own query\n",
        "SAMPLE_QUERY = \"Select the name of the employee who has the highest salary\"\n",
        "\n",
        "prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(\n",
        "    query=SAMPLE_QUERY\n",
        ")\n",
        "\n",
        "result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "\n",
        "# # Do try to investigate the generations from LangChain to understand what is generated\n",
        "print(result.generations[0][0].text)"
      ],
      "metadata": {
        "id": "xcLCb77HbZ1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job! We have generated our very first SQL code using an LLM!\n",
        "\n",
        "An improvement we can perform on this is using a **Few-Shot prompt** where we include a couple of examples for the LLM to understand what we are trying to do. Doing so should result in better results as the LLM has some reference examples about what we are trying to do.\n",
        "\n",
        "**BUT** remember *we need to evaluate our LLM* prompt before we can even think about experimenting on it otherwise how would we even know if this new prompt is even better in the first place?"
      ],
      "metadata": {
        "id": "ys1WKl5JdIoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Evaluating LLMs on SQL Code Generation\n",
        "\n",
        "Evaluating Text-to-SQL might seem like an easy tasks to evaluate as all we need in order to generate our Golden Dataset would be a list of natural language queries and their respective SQL query but there are some intricacies with it that will prove to be challenging.\n",
        "\n",
        "We will need some way to check for **equivalence** to the **gold** query, which is where prior mentioned intricacies will start to pop up, but we will circle back to them later on. With that, we can start off with looking for an exact match between the LLM response and the golden query. For our initial metric we will be using a very simple metric, [exact match](https://huggingface.co/spaces/evaluate-metric/exact_match).\n",
        "\n",
        "![Basic Exact Match LLM Evaluator](https://drive.google.com/uc?id=1yIygL2zNC4JZ02dMBtC5HyNa0_ujIOOv)\n",
        "\n",
        "With a Golden Dataset which we can hand generate using our domain expertise, we can build up an evaluation dataset to run on every iteration to improve our application. Using the exact match metric we can derive an accuracy score of our LLM-application that we can keep track of and use in order to compare the performance across different iterations overtime.\n",
        "\n",
        "Here is an example of what our dataset can look like:\n",
        "\n",
        "![Example Golden Dataset](https://drive.google.com/uc?id=12D8C2juErtOzvKVLaKL_njxlceFJcbTu)\n",
        "\n",
        "**NOTE:** If you noticed, there is a **complexity** column on the SQL code used in the example which we can try incorporating as an optional tasks in order to improve the quality of our results across different fine-grained categories which is paramount in ensuring our LLM SQL Code Generator is more robust. Ideally we should have an even distribution across all these additional categories that we add on, where we can then calculate metrics such as **Accuracy@HardComplexity** and so on.\n",
        "\n",
        "We will provide you with a basic evaluation dataset to start off with, which we can further improve on using the process laid out below using [deepeval](https://github.com/confident-ai/deepeval):\n",
        "\n",
        "![Evaluation Process](https://drive.google.com/uc?id=1KC_gMvU3PeflEm0N1Co8_YnT46lCvMLU)\n",
        "<!-- 1. Load a Golden Test set for the students in a JSONL file with 20-30 examples\n",
        "2. TODO: Create a custom deep-eval evaluator for SQL code where we check if the output of the SQL code exactly matches the output from the LLM.\n",
        "3. TODO: Run evaluation on the golden test set.\n",
        "4. TODO: Try different prompts and see if they improve the evaluation metric\n",
        "5. Checking for an exact match is not the best way to go about it as the same query could have multiple ways to do it. Let's try using G-Eval where we use another more powerful LLM.\n",
        "6. TODO: Evaluate using G-Eval\n",
        "7. Talk about how in a Production system we should be running evaluation just like how we run automated tests, via CI/CD pipelines. -->"
      ],
      "metadata": {
        "id": "pWz1btwOM7zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASIC_GROUND_TRUTH_DATASET = [\n",
        "  {\n",
        "    \"Query\": \"Show the total number of employees.\",\n",
        "    \"Ground Truth\": \"SELECT COUNT(*) AS total_employees FROM employees;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Find the highest salary in the Finance department.\",\n",
        "    \"Ground Truth\": \"SELECT MAX(salary) FROM employees WHERE department = 'Finance';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Get the average age of all managers.\",\n",
        "    \"Ground Truth\": \"SELECT AVG(age) FROM employees WHERE position = 'Manager';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List the names and emails of staff in the IT department.\",\n",
        "    \"Ground Truth\": \"SELECT name, email FROM employees WHERE department = 'IT';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"What are the titles of the top 5 selling books?\",\n",
        "    \"Ground Truth\": \"SELECT title FROM books ORDER BY sales DESC LIMIT 5;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Which product has the least quantity in stock?\",\n",
        "    \"Ground Truth\": \"SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Display the second highest salary in the organization.\",\n",
        "    \"Ground Truth\": \"SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List employees who joined after 2015 and work in the Sales department.\",\n",
        "    \"Ground Truth\": \"SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Find the average order value for each customer.\",\n",
        "    \"Ground Truth\": \"SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Show departments that have more than 10 employees.\",\n",
        "    \"Ground Truth\": \"SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List all products that have never been ordered.\",\n",
        "    \"Ground Truth\": \"SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Which customers have spent more than $1000 in total?\",\n",
        "    \"Ground Truth\": \"SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Show the total number of orders placed each day last week.\",\n",
        "    \"Ground Truth\": \"SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List the names of employees who do not manage anyone.\",\n",
        "    \"Ground Truth\": \"SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Display the department that has the highest average employee salary.\",\n",
        "    \"Ground Truth\": \"SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "dKEiy2NmToCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(BASIC_GROUND_TRUTH_DATASET)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JSrkIWZETx2W",
        "outputId": "c12d706a-97f4-42b8-cea5-f2f102babeb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Query  \\\n",
              "0                Show the total number of employees.   \n",
              "1  Find the highest salary in the Finance departm...   \n",
              "2               Get the average age of all managers.   \n",
              "3  List the names and emails of staff in the IT d...   \n",
              "4    What are the titles of the top 5 selling books?   \n",
              "\n",
              "                                        Ground Truth  \n",
              "0  SELECT COUNT(*) AS total_employees FROM employ...  \n",
              "1  SELECT MAX(salary) FROM employees WHERE depart...  \n",
              "2  SELECT AVG(age) FROM employees WHERE position ...  \n",
              "3  SELECT name, email FROM employees WHERE depart...  \n",
              "4  SELECT title FROM books ORDER BY sales DESC LI...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57b2de1d-a58f-4539-b223-3d835db709ba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Ground Truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Show the total number of employees.</td>\n",
              "      <td>SELECT COUNT(*) AS total_employees FROM employ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Find the highest salary in the Finance departm...</td>\n",
              "      <td>SELECT MAX(salary) FROM employees WHERE depart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Get the average age of all managers.</td>\n",
              "      <td>SELECT AVG(age) FROM employees WHERE position ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>List the names and emails of staff in the IT d...</td>\n",
              "      <td>SELECT name, email FROM employees WHERE depart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the titles of the top 5 selling books?</td>\n",
              "      <td>SELECT title FROM books ORDER BY sales DESC LI...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57b2de1d-a58f-4539-b223-3d835db709ba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57b2de1d-a58f-4539-b223-3d835db709ba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57b2de1d-a58f-4539-b223-3d835db709ba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a522e501-d36e-4477-bd7b-c9c426c96724\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a522e501-d36e-4477-bd7b-c9c426c96724')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a522e501-d36e-4477-bd7b-c9c426c96724 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Show departments that have more than 10 employees.\",\n          \"Which customers have spent more than $1000 in total?\",\n          \"Show the total number of employees.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ground Truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\",\n          \"SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\",\n          \"SELECT COUNT(*) AS total_employees FROM employees;\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, lets see how our LLM and Prompt are able to perform on the entire dataset by running it through and evaluating its performance. This is known as performing a LLM Task Evaluation which is different than LLM Model Evals. Task evaluation cares about the performance of the LLM on a specific task whereas Model evals care about the general performance of the LLM across a variety of tasks, benchmarks such as the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset is an example of a Model Evals and Text2SQL accuracy like we are doing in this project is an example of task evaluation.\n",
        "\n",
        "We will start off with traditional NLP metrics that compare the output with the ground truth can derive a score based on the exact/similarity of the two. These metrics are simple to use and serve as a good place to start off when evaluating LLMs.\n",
        "\n",
        "Firstly, let's start off with the exact match metric which very simply just compares the output with the ground truth and checks whether its an exact match or not, deepeval implements these metrics within the [Scorer module](https://github.com/confident-ai/deepeval/blob/4b3ceed20993232331550798fe0a8f1bf2605594/deepeval/scorer/scorer.py#L7).\n",
        "\n",
        "deepeval makes it easy to create our own metrics by inheriting the BaseMetric class, you can read more about creating custom metrics [here](https://docs.confident-ai.com/docs/metrics-custom)."
      ],
      "metadata": {
        "id": "qBOyfpn54-1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import BaseMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.scorer import Scorer\n",
        "\n",
        "\n",
        "class ExactMatchMetric(BaseMetric):\n",
        "    def __init__(self, threshold: float = 0.0, async_mode: bool = True):\n",
        "        self.threshold = threshold\n",
        "        self.async_mode = async_mode\n",
        "\n",
        "    def _base_measure(self, test_case: LLMTestCase):\n",
        "        self.success = Scorer.exact_match_score(\n",
        "            test_case.actual_output, test_case.expected_output\n",
        "        )\n",
        "        if self.success:\n",
        "            self.score = 1\n",
        "        else:\n",
        "            self.score = 0\n",
        "        return self.score\n",
        "\n",
        "    def measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    async def a_measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    def is_successful(self):\n",
        "        return bool(self.success)\n",
        "\n",
        "    @property\n",
        "    def __name__(self):\n",
        "        return \"ExactMatch\""
      ],
      "metadata": {
        "id": "PPOcKnpcUlD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create our first test case!\n",
        "\n",
        "test_case_input = df.head(1)\n",
        "test_case_query = test_case_input['Query'].squeeze()\n",
        "test_case_expected_output = test_case_input['Ground Truth'].squeeze()"
      ],
      "metadata": {
        "id": "5apnNHGFYvno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Here is our query: {test_case_query}\")\n",
        "print(f\"Here is our ground truth: {test_case_expected_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrLlDUKMb7oq",
        "outputId": "acff6dbb-a6f4-4c8d-cbfa-5046b5920ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is our query: Show the total number of employees.\n",
            "Here is our ground truth: SELECT COUNT(*) AS total_employees FROM employees;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instruction:\n",
        "\n",
        "${query}\n",
        "\n",
        "Only generate SQL code and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(\n",
        "    query=test_case_query\n",
        ")\n",
        "\n",
        "result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "\n",
        "test_case_actual_output = result.generations[0][0].text"
      ],
      "metadata": {
        "id": "7twvbNfOaMaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Here is what our model generated: {test_case_actual_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue1bMG-ubEpt",
        "outputId": "b6df2930-6778-492c-c411-ea61ce8850cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is what our model generated: ```sql\n",
            "SELECT COUNT(*) AS total_employees FROM employees;\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "  input=test_case_input,\n",
        "  actual_output=\"SELECT COUNT(*) AS total_employees FROM employees;\",\n",
        "  expected_output=test_case_expected_output\n",
        ")"
      ],
      "metadata": {
        "id": "9FQdZAxXZrQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match_metric = ExactMatchMetric()\n",
        "exact_match_metric.measure(test_case)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oQx25b7ctYh",
        "outputId": "8f65246f-8dc6-4619-9813-d06b0f06f02e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test case: {exact_match_metric.is_successful()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM2s34m-dBKW",
        "outputId": "4656ffee-bfbd-4038-da68-1833e0295e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test case: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** If your prompt still fails and produces a different SQL code such as *\"SELECT COUNT(*) AS total_employees FROM employees;\"* feel free to update the ground truth to get it to pass in the meantime. This can happen as code correctness is not a simple task due to the fact that different implementations can still solve the query which is something we invite you to address via the optional tasks for this week.\n",
        "\n",
        "\n",
        "Great job! We are able to see that our LLM and Prompt are able to answer the test case correctly, but that is just a basic example that we have. Next let us run it on the entire dataset that we have created.\n",
        "\n",
        "## TODO: Create a deepeval dataset and evalute our LLM and Prompt on it\n",
        "\n",
        "There is a slight difference between what we have been learning about Golden Datasets in general and how Golden Datasets work within deepeval. When we talk about a Golden Dataset as per the [course materials here](https://uplimit.com/course/llms-in-production/v2/module/evaluating-llms#corise_clrnjsvpd001m2e6iogarwdpn) we mention that its similar to a held-out test set meaning that we have data to predict on alongside their ground truth labels in order to derive evaluation metrics.\n",
        "\n",
        "In deepeval, we will need to create an Evaluation Dataset instead where an Evaluation Dataset is built from LLMTestCase's and/or Golden's. Remember that the `actual_output` comes from running our LLM against the `input` as we did above!\n",
        "```python\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "first_test_case = LLMTestCase(input=\"...\", actual_output=\"...\")\n",
        "second_test_case = LLMTestCase(input=\"...\", actual_output=\"...\")\n",
        "\n",
        "test_cases = [first_test_case, second_test_case]\n",
        "dataset = EvaluationDataset(test_cases=test_cases)\n",
        "```\n",
        "The definition of Golden's within deepeval is documented [here](https://docs.confident-ai.com/docs/confident-ai-manage-datasets#what-is-a-golden):\n",
        "```\n",
        "A \"Golden\" is what makes up an evaluation dataset and is very similar to a test case in deepeval, but they:\n",
        "\n",
        "    - do not require an actual_output, so whilst test cases are always ready for evaluation, a golden isn't.\n",
        "    - only exists within an EvaluationDataset(), while test cases can be defined anywhere.\n",
        "    - contains an extra additional_metadata field, which is a dictionary you can define on Confident. Allows you to do some extra preprocessing on your dataset (eg., generating a custom LLM actual_output based on some variables in additional_metadata) before evaluation.\n",
        "\n",
        "We introduced the concept of goldens because it allows you to create evaluation datasets on Confident without needing pre-computed actual_outputs. This is especially helpful if you are looking to generate responses from your LLM application at evaluation time.\n",
        "```\n",
        "With our EvaluationDataset created we can then run our LLM through it in order to derive the final score. For example, the following does so without using Pytest although its recommended that you treat this process as a CI/CD process that runs alongside your Pytest suite for your actual application.\n",
        "\n",
        "```python\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "dataset = EvaluationDataset(test_cases=[...])\n",
        "hallucination_metric = HallucinationMetric(threshold=0.3, context=[\"...\"])\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "dataset.evaluate([hallucination_metric, answer_relevancy_metric])\n",
        "\n",
        "# You can also call the evaluate() function directly\n",
        "evaluate(dataset, [hallucination_metric, answer_relevancy_metric])\n",
        "```\n",
        "An example output of the evaluation would look like this:\n",
        "```\n",
        "======================================================================\n",
        "\n",
        "Metrics Summary\n",
        "\n",
        "  - ✅ Answer Relevancy (score: 1, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 1.00 because the response perfectly addresses the question about operating hours without any irrelevant information. Great job!, error: None)\n",
        "  - ✅ Bias (score: 0, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 0.00 because the output maintains neutrality and objectivity, as evidenced by the absence of any cited biased phrases or issues., error: None)\n",
        "\n",
        "For test case:\n",
        "\n",
        "  - input: What are your operating hours?\n",
        "  - actual output: ...\n",
        "  - expected output: None\n",
        "  - context: ['Our company operates from 10 AM to 6 PM, Monday to Friday.', 'We are closed on weekends and public holidays.', 'Our customer service is available 24/7.']\n",
        "  - retrieval context: None\n",
        "\n",
        "======================================================================\n",
        "\n",
        "Metrics Summary\n",
        "\n",
        "  - ✅ Answer Relevancy (score: 1, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 1.00 because the response accurately addresses the question about free shipping without any irrelevant information. Great job on maintaining focus!, error: None)\n",
        "  - ✅ Bias (score: 0, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 0.00 because the actual output perfectly demonstrates neutrality and objectivity, without any indication of bias., error: None)\n",
        "\n",
        "For test case:\n",
        "\n",
        "  - input: Do you offer free shipping?\n",
        "  - actual output: ...\n",
        "  - expected output: Yes, we offer free shipping on orders over $50.\n",
        "  - context: None\n",
        "  - retrieval context: None\n",
        "\n",
        "======================================================================\n",
        "```\n",
        "One thing to note is that the metrics are run on a single (query, ground_truth) instead across the entire dataset meaning that we lack the ability to have a singular number to compare across different evaluation runs like traditional machine learning where you would for example have a accuracy score for the test set.\n",
        "\n",
        "Let us introduce the concept of pass rates which are similar to accuracy across the entire dataset but its just meant to show how many instances for a metric in the dataset passed a evaluation metric check.\n",
        "\n",
        "With this you could enforce a strict pass rate on deployments much like how we can enforce Pytest code coverage to be off a certain %.\n",
        "\n",
        "deepeval supports this via the `aggregate_metric_pass_rates()` on the results of the evaluation give us pass rates across each metric that we use:\n",
        "\n",
        "```python\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "\n",
        "evaluation_results = evaluate(dataset, [hallucination_metric, answer_relevancy_metric])\n",
        "aggregate_metric_pass_rates(evaluation_results.test_results)\n",
        "```\n",
        "\n",
        "This should output the following where we get the pass rates across each metric and a mapping storing the pass rates across each metric which we can use to store and compare across different evaluation runs:\n",
        "```\n",
        "======================================================================\n",
        "\n",
        "Aggregate Metric Pass Rates\n",
        "\n",
        "AnswerRelevancyMetric: 100.00% pass rate\n",
        "BiasMetric: 100.00% pass rate\n",
        "\n",
        "======================================================================\n",
        "\n",
        "{'AnswerRelevancyMetric': 1.0, 'BiasMetric': 1.0}\n",
        "```\n",
        "\n",
        "Now with this you can apply it to our dataset:\n",
        "1. Create an [EvaluationDataset](https://docs.confident-ai.com/docs/evaluation-datasets#create-an-evaluation-dataset) using deepeval.\n",
        "2. Run the evaluation on the Golden Dataset without Pytest, refer to [this](https://docs.confident-ai.com/docs/evaluation-introduction#evaluating-without-pytest)."
      ],
      "metadata": {
        "id": "n-A23MUVfNhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################"
      ],
      "metadata": {
        "id": "RJ_wARozNj82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good job evaluating the LLM and Prompt on the entire dataset that we have created! If you notice it isn't that good at this task still. What do you think we can do to improve the evaluation performance? What issues do you see with the way we perform evaluations at this point of time?\n",
        "- TODO: ADD YOUR THOUGHTS HERE"
      ],
      "metadata": {
        "id": "7GMJSp2C8jVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Validating LLM Outputs using Guardrails\n",
        "\n",
        "When building our LLM Application one difficult thing is the fact that LLM's are non-deterministic in their outputs. We are able to control different parameters within our LLM to mitigate this to a certain extent such as:\n",
        "1. temperature / top_p\n",
        "2. seed\n",
        "\n",
        "You can read more about these parameters from OpenAI's documentation [here](https://platform.openai.com/docs/api-reference/chat/create).\n",
        "\n",
        "But ensuring consistent outputs is still a problem. *This is where having guardrails comes into play.*\n",
        "\n",
        "> Guardrails = safety controls for LLMs\n",
        "\n",
        "Just like guardrails help prevent cars from going off the road, they can also help out with making sure LLM outputs are in line with set expectations.\n",
        "\n",
        "![Road guardrails](https://drive.google.com/uc?id=1KZCfJs9Sf6ExqpphnTa4G-AaisxFBRWG)\n",
        "\n",
        "Guardrails can take many different forms which differ in complexity:\n",
        "1. **Security Guardrails**, ensuring there isn't any PII data in the response\n",
        "2. **Compliance Guardrails**, ensuring that competitors are not talked about\n",
        "3. **Safety Guardrails**, ensuring that there are not toxic responses\n",
        "4. **Structural Guardrails**, ensuring that the response matches a specific JSON schema\n",
        "\n",
        "\n",
        "A simple Guardrail for this situation could be to ensure that there are **no default error results incorporated within the product description**.\n",
        "\n",
        "In our application we will also be communicating via REST APIs which leverage JSON to send and receive data between different applications.\n",
        "\n",
        "**Frontend JSON sent to the Backend:**\n",
        "```json\n",
        "{\n",
        "    \"prompt\": \"Write a SQL query to find the names and email addresses of all users in the 'users' table who joined after January 1, 2020.\",\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 100\n",
        "}\n",
        "```\n",
        "**Backend Response JSON sent back to the Frontend:**\n",
        "```json\n",
        "{\n",
        "  \"id\": \"cmpl-XYZ456\",\n",
        "  \"object\": \"text_completion\",\n",
        "  \"created\": 1616517999,\n",
        "  \"model\": \"gpt-3.5-turbo\",\n",
        "  \"prediction\": {\n",
        "    \"text\": \"SELECT name, email FROM users WHERE join_date > '2020-01-01';\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "The response from OpenAI will be used to populate the `prediction` field within the main Backend Response where we will have an object that contains a `text` field which will then be used to contain the actual output from OpenAI.\n",
        "\n",
        "In this scenario we would initially look for a **Structural Guardrail** that ensures that the text response from the LLM:\n",
        "1. *is a string*\n",
        "2. *is not empty*\n",
        "\n",
        "A great library for this would be [Guardrails.ai](https://www.guardrailsai.com/) which allows us to leverage [Pydantic](https://docs.pydantic.dev/latest/) in order to easily define validators for our LLM responses.\n"
      ],
      "metadata": {
        "id": "rPmPTQssNDUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!guardrails configure --token $GUARDRAILS_TOKEN"
      ],
      "metadata": {
        "id": "ZSEG6jxaaJFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class LLMResponse(BaseModel):\n",
        "  generated_sql: str = Field(description=\"Generated SQL from LLM\")"
      ],
      "metadata": {
        "id": "JUf5Y8tizDdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from rich import print\n",
        "from guardrails import Guard\n",
        "\n",
        "# TODO: Add in your prompt here from before along with the query\n",
        "# NOTE: gr.complete_json_suffix_v2 comes from guardrail where it will inject\n",
        "#       additional tokens related to enforcing the Pydantic Model, to the main prompt.\n",
        "ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE = \"\"\"\n",
        "${query}\n",
        "\n",
        "${gr.complete_json_suffix_v3}\n",
        "\"\"\"\n",
        "\n",
        "guard = Guard.from_pydantic(output_class=LLMResponse, prompt=ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE)\n",
        "\n",
        "raw_llm_output, validated_output, *rest = guard(\n",
        "    llm_api=openai.chat.completions.create,\n",
        "    model=MODEL_NAME,\n",
        "    prompt_params={\n",
        "        \"query\": SAMPLE_QUERY\n",
        "    },\n",
        ")\n"
      ],
      "metadata": {
        "id": "6E10suedyZwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(guard.history.last.prompt)"
      ],
      "metadata": {
        "id": "PRoWerMjB1um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_llm_output)"
      ],
      "metadata": {
        "id": "utarYL7rTmcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "36807084-ad48-4f72-d012-d2a400824a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m\"generated_sql\"\u001b[0m: \u001b[32m\"SELECT name FROM employee ORDER BY salary DESC LIMIT 1\"\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"generated_sql\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"SELECT name FROM employee ORDER BY salary DESC LIMIT 1\"</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(validated_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "otmh76saBwHz",
        "outputId": "d96af2e1-ecb8-4e82-cd6c-cf809e0a4c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m'generated_sql'\u001b[0m: \u001b[32m'SELECT name FROM employee ORDER BY salary DESC LIMIT 1'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT name FROM employee ORDER BY salary DESC LIMIT 1'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can view the history of our prompts in a nice tree like format\n",
        "print(guard.history.last.tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "NMhpA6jtDlLJ",
        "outputId": "a20f8cf8-d440-47a2-f1f8-125dbc524a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Logs\n",
              "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
              "    │ \u001b[48;2;240;248;255m╭─\u001b[0m\u001b[48;2;240;248;255m───────────────────────────────────────────────\u001b[0m\u001b[48;2;240;248;255m Prompt \u001b[0m\u001b[48;2;240;248;255m────────────────────────────────────────────────\u001b[0m\u001b[48;2;240;248;255m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m    Generate a valid SQL query for the following natural language instruction:\u001b[0m\u001b[48;2;240;248;255m                         \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m    \u001b[0m\u001b[48;2;240;248;255m                                                                                                   \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m    Select the name of the employee who has the highest salary\u001b[0m\u001b[48;2;240;248;255m                                         \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m    \u001b[0m\u001b[48;2;240;248;255m                                                                                                   \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mGiven below is XML that describes the information to extract from this document and the tags to extract\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mit into.\u001b[0m\u001b[48;2;240;248;255m                                                                                               \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m<output>\u001b[0m\u001b[48;2;240;248;255m                                                                                               \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m    <string name=\"generated_sql\" description=\"Generated SQL from LLM\"/>\u001b[0m\u001b[48;2;240;248;255m                                \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m</output>\u001b[0m\u001b[48;2;240;248;255m                                                                                              \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`name` attribute of the corresponding XML, and the value is of the type specified by the corresponding \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mXML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. \u001b[0m\u001b[48;2;240;248;255m      \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mrequests for lists, objects and specific types. Be correct and concise.\u001b[0m\u001b[48;2;240;248;255m                                \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mHere are examples of simple (XML, JSON) pairs that show the expected behavior:\u001b[0m\u001b[48;2;240;248;255m                         \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- `<string name='foo' format='two-words lower-case' />` => `{'foo': 'example one'}`\u001b[0m\u001b[48;2;240;248;255m                    \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- `<list name='bar'><string format='upper-case' /></list>` => `{\"bar\": ['STRING ONE', 'STRING TWO', \u001b[0m\u001b[48;2;240;248;255m   \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255metc.]}`\u001b[0m\u001b[48;2;240;248;255m                                                                                                \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- `<object name='baz'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" \u001b[0m\u001b[48;2;240;248;255m        \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mformat=\"1-indexed\" /></object>` => `{'baz': {'foo': 'Some String', 'index': 1}}`\u001b[0m\u001b[48;2;240;248;255m                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m╭─\u001b[0m\u001b[48;2;255;240;242m────────────────────────────────────────────\u001b[0m\u001b[48;2;255;240;242m Instructions \u001b[0m\u001b[48;2;255;240;242m─────────────────────────────────────────────\u001b[0m\u001b[48;2;255;240;242m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m│\u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242mYou are a helpful assistant, able to express yourself purely through JSON, strictly and precisely \u001b[0m\u001b[48;2;255;240;242m     \u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242m│\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m│\u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242madhering to the provided XML schemas.\u001b[0m\u001b[48;2;255;240;242m                                                                  \u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242m│\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m╭─\u001b[0m\u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m\u001b[48;2;231;223;235m Message History \u001b[0m\u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m\u001b[48;2;231;223;235m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m│\u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235mNo message history.\u001b[0m\u001b[48;2;231;223;235m                                                                                    \u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m│\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m╭─\u001b[0m\u001b[48;2;245;245;220m───────────────────────────────────────────\u001b[0m\u001b[48;2;245;245;220m Raw LLM Output \u001b[0m\u001b[48;2;245;245;220m────────────────────────────────────────────\u001b[0m\u001b[48;2;245;245;220m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m{\u001b[0m\u001b[48;2;245;245;220m                                                                                                      \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m    \"generated_sql\": \"SELECT name FROM employee ORDER BY salary DESC LIMIT 1\"\u001b[0m\u001b[48;2;245;245;220m                          \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m}\u001b[0m\u001b[48;2;245;245;220m                                                                                                      \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m╭─\u001b[0m\u001b[48;2;240;255;240m──────────────────────────────────────────\u001b[0m\u001b[48;2;240;255;240m Validated Output \u001b[0m\u001b[48;2;240;255;240m───────────────────────────────────────────\u001b[0m\u001b[48;2;240;255;240m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m{'generated_sql': 'SELECT name FROM employee ORDER BY salary DESC LIMIT 1'}\u001b[0m\u001b[48;2;240;255;240m                            \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Logs\n",
              "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
              "    │ <span style=\"background-color: #f0f8ff\">╭──────────────────────────────────────────────── Prompt ─────────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│     Generate a valid SQL query for the following natural language instruction:                          │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│     Select the name of the employee who has the highest salary                                          │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Given below is XML that describes the information to extract from this document and the tags to extract │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ it into.                                                                                                │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ &lt;output&gt;                                                                                                │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│     &lt;string name=\"generated_sql\" description=\"Generated SQL from LLM\"/&gt;                                 │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ &lt;/output&gt;                                                                                               │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g.        │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ requests for lists, objects and specific types. Be correct and concise.                                 │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Here are examples of simple (XML, JSON) pairs that show the expected behavior:                          │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ - `&lt;string name='foo' format='two-words lower-case' /&gt;` =&gt; `{'foo': 'example one'}`                     │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ - `&lt;list name='bar'&gt;&lt;string format='upper-case' /&gt;&lt;/list&gt;` =&gt; `{\"bar\": ['STRING ONE', 'STRING TWO',     │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ etc.]}`                                                                                                 │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ - `&lt;object name='baz'&gt;&lt;string name=\"foo\" format=\"capitalize two-words\" /&gt;&lt;integer name=\"index\"          │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ format=\"1-indexed\" /&gt;&lt;/object&gt;` =&gt; `{'baz': {'foo': 'Some String', 'index': 1}}`                        │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">╭───────────────────────────────────────────── Instructions ──────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">│ You are a helpful assistant, able to express yourself purely through JSON, strictly and precisely       │</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">│ adhering to the provided XML schemas.                                                                   │</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">╭──────────────────────────────────────────── Message History ────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">│ No message history.                                                                                     │</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">╭──────────────────────────────────────────── Raw LLM Output ─────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">│ {                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">│     \"generated_sql\": \"SELECT name FROM employee ORDER BY salary DESC LIMIT 1\"                           │</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">│ }                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">╭─────────────────────────────────────────── Validated Output ────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│ {'generated_sql': 'SELECT name FROM employee ORDER BY salary DESC LIMIT 1'}                             │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Adding More Guardrails\n",
        "We have created a very basic Guardrail that leverages Pydantic Model's to ensure that our response fits a specifc format. Now let us try more advanced guardrails:\n",
        "- Try creating **Bug Free SQL Code** leveraging the following [example](https://www.guardrailsai.com/docs/examples/syntax_error_free_sql).\n",
        "- Feel free to add in more [guardrails via the Guardrails.ai Hub](https://hub.guardrailsai.com/) where you see fit for our use case. For example, we could ensure that there isn't any toxic language being returned from out LLM via using the [ToxicLanguage Validator](https://www.guardrailsai.com/docs/examples/toxic_language).\n",
        "- How should we handle validation failures? One way would be to leverage reasking, where we reask the LLM to fix the output from a validation check failure. Within Guardrails.ai there are corrective measures we can take to handle validation errors as outlined [here](https://www.guardrailsai.com/docs/how_to_guides/rail#%EF%B8%8F-specifying-corrective-actions).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KtDOZyPK2D4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################"
      ],
      "metadata": {
        "id": "XQ-2FIj2NnaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job adding in robust guardrails, next we can now focus on making sure our system performs at peak performance. Let us try improving the results of the model using Prompt Engineering techniques making sure that we compare results from the baseline with the new improvements. Feel free to go wild and experiment with all kinds of techniques for this section!\n",
        "\n",
        "## TODO: Evaluation Driven Development(EDD)\n",
        "\n",
        "EDD is similar to [Test Driven Development(TDD)](https://martinfowler.com/bliki/TestDrivenDevelopment.html) except we apply it to building LLM-powered applications, where our test cases in this case are derived from the Golden dataset that we created earlier and the goal here is to iteratively ensure that our LLM application can achieve a **100% pass rate** across all the examples.\n",
        "\n",
        "The main idea here is that if we currently have a 60% pass rate on the exact match metric, we should take an iterative approach towards increasing the pass rate. For example, the LLM could be failing on the following query `Display the department that has the highest average employee salary.`. In this section we will try to tune the prompt or other parts to ensure that the model is able to correctly generate the ground truth for it.\n",
        "\n",
        "NOTE: As we mentioned before some items from the dataset could result in false negatives since multiple implementations are possible for the same query.\n",
        "\n",
        "Here are a couple of things we can try out:\n",
        "- Try out Few-Shot Prompting and [other techniques](https://www.promptingguide.ai/techniques) and see how it impacts our evaluation performance.\n",
        "- Try tuning the parameters used for a model.\n",
        "- Try out different LLM models from OpenAI, Anthropic, etc. if you have API keys for them.\n",
        "\n",
        "You will have to setup an EvaluationDataset per each comparison since each run would have a different `actual_output` used within the LLMTestCase.\n",
        "```python\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "dataset_run_1 = EvaluationDataset(test_cases=[...])\n",
        "dataset_run_2 = EvaluationDataset(test_cases=[...])\n",
        "\n",
        "evaluation_results_run_1 = evaluate(dataset_run_1, [exact_match_metric])\n",
        "aggregate_metric_pass_rates(evaluation_results_run_1.test_results)\n",
        "\n",
        "evaluation_results_run_2 = evaluate(dataset_run_2, [exact_match_metric])\n",
        "aggregate_metric_pass_rates(evaluation_results_run_2.test_results)\n",
        "\n",
        "assert evaluation_results_run_2.test_results[0].metrics_data[0].score > evaluation_results_run_1.test_results[0].metrics_data[0].score, \\\n",
        "  \"Run 2 does not perform better than Run 1\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "VgWBsMZ-Ex96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################"
      ],
      "metadata": {
        "id": "fBl8t9QZNovv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Evaluating SQL Code Using a Stronger LLM\n",
        "\n",
        "Usually we would prioritise latency during inference using LLMs for most use cases, LLMs such as `gpt3.5-turbo` are much faster than `gpt4` variants while performing worse in general across benchmarks. What if we could also use a bigger and stronger LLM to aid with evaluations as well? There has been a rise of techniques doing just that where a smaller and faster LLM is evaluated by a bigger and stronger LLM. This process helps automate the evaluation part of any LLM-powered application! This is kinda of like having a dream within a dream just like in the movie *Inception*.\n",
        "\n",
        "![Dream within a dream from Inception](https://drive.google.com/uc?id=1EC7rNg_LCLjIJ9SRyQpGVgtplOHZllDm)\n",
        "\n",
        "\n",
        "**NOTE:** LLMs are very powerful and can help automate so many parts of the process but becareful not to be too reliant on them, at the end of the day they aren't perfect and hallucinate a lot. Therefore, its vital that you always keep a human-in-the-loop to verify things as a final seal of approval.\n",
        "\n",
        "\n",
        "### Introducing G-Eval\n",
        "\n",
        "[G-Eval](https://arxiv.org/abs/2303.16634) is a method that utilizes LLMs alongside a Chain-of-Thought (CoT) and form-filling approach to assess the outputs of LLMs. Initially, a task outline and evaluation parameters are provided to an LLM, which is then requested to create a CoT detailing the evaluation steps. For assessing the coherence of news summaries, the prompt, CoT, news article, and summary are merged, and the LLM is instructed to produce a score ranging from 1 to 5. Subsequently, the output token probabilities generated by the LLM are used to standardize this score, with a weighted summation taken as the ultimate outcome.\n",
        "\n",
        "It was observed that using GPT-4 as an evaluator resulted in a high Spearman correlation with human assessments (0.514), surpassing all former methods. In the realm of summarization, it exceeded all prior state-of-the-art (SOTA) evaluators in the SummEval benchmark, which evaluates coherence, consistency, fluency, and relevance.\n",
        "\n",
        "\n",
        "![G-Eval](https://drive.google.com/uc?id=1dq1BAjLY_Es5r-UiD96ZnFlxWTYcNv8g)\n",
        "\n",
        "deepeval already comes out of the box with support for using G-Eval [here](https://docs.confident-ai.com/docs/metrics-llm-evals) where you can very easily just create a new metric and run it against your EvaluationDataset:\n",
        "\n",
        "```python\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "correctness_metric = GEval(\n",
        "    name=\"Correctness\",\n",
        "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
        "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
        "    evaluation_steps=[\n",
        "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "        \"You should also heavily penalize omission of detail\",\n",
        "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
        "    ],\n",
        "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        ")\n",
        "```\n",
        "Try creating a G-Eval metric for SQL Code Generation and running it through our EvaluationDataset like before.\n",
        "\n",
        "\n",
        "**NOTE:** The API Key provided to you only allows access to `gpt-4o-mini`, we can still use it either way but in the real world you would ideally use a more powerful model for this task. If you were to use `gpt-4o` as your base model then you probably don't have another *stronger* LLM to use therefore this also mimics that kind of constraint."
      ],
      "metadata": {
        "id": "pnh0hFV-HLK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################"
      ],
      "metadata": {
        "id": "VgazsrdbRouA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job on getting G-Eval to work with our dataset! What do you think about using LLMs to evaluate another LLM vs just using traditional Machine Learning metrics? What are the Pros/Cons of it and where do you think it would work better or worse in?\n",
        "\n",
        "**TODO: Add your thoughts here!**"
      ],
      "metadata": {
        "id": "HC6hxGkHRuEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Extra Credit: Optional Tasks\n",
        "\n",
        "- Try out implementing the Execution Score from the Snowflake team and comparing the results of it to Exact Match Metric and G-Eval, this [blog](https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-1a33b3ee0d37) from the Snowflake Team covers how you can use LLMs to assists with evaluation.\n",
        "- If you use SQL enough you would know that there are multiple ways in which we can answer a particular query especially if you had to update your ground truth earlier. With that you can try out other frameworks such as [sql-eval](https://github.com/defog-ai/sql-eval) which provides mo like we did before but more importanly also includes code execution into context when evaluating LLMs.\n",
        "- Guardrails do not just apply to the output from an LLM, instead we can also apply them to the actual input that a user feeds into the LLM. You can try out **Detecting Toxicity**, **Restricting the input to just be related to SQL code generation**, **Limiting the amount of tokens used**, etc.\n",
        "- Our BASIC_GROUND_TRUTH_DATASET is quite small, see how you can add in more examples using your own domain knowledge or you could even leverage an LLM to do so via synthetic data generation which is supported within deepeval [here](https://docs.confident-ai.com/docs/evaluation-datasets-synthetic-data). Remember not all users will put in queries that can be answered with SQL code. For example, if the user asks `Who will win the next US Presidential Election?` you definitely don't want to answer it at all!\n"
      ],
      "metadata": {
        "id": "UL-XI8PiNU1z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIcvjLeVSEeD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}